# -*- coding: utf-8 -*-
"""
Spyder Editor

This is a temporary script file.
"""
#importing all the required modules(packages)
import nltk, re, pprint
from nltk import word_tokenize
from urllib import request
from nltk import sent_tokenize
from nltk.corpus import stopwords
import numpy as np
import random

##url1 Benjamin Franklin Autobiography
url = "http://www.gutenberg.org/cache/epub/13033/pg13033.txt"
response = request.urlopen(url)
raw = response.read().decode('utf8')
#stripping the text
raw1=raw.rstrip()
#converting the text into lower case
raw1=raw1.lower()
#removing all the stopwords
stop_words = set(stopwords.words('english')) 
#performing tokenisation
word_tokens = word_tokenize(raw1) 
filtered_sentence = [w for w in word_tokens if not w in stop_words] 
filtered_sentence = [] 
for w in word_tokens: 
    if w not in stop_words: 
        filtered_sentence.append(w) 
words = [word for word in filtered_sentence if word.isalpha()]
from nltk.tokenize.treebank import TreebankWordDetokenizer as Detok
detokenizer = Detok()
words2=detokenizer.detokenize(words)
x1=[]

#For Loop for the segmentation into 200 rows and 150 columns
for i in range (0,200*150,150):
     x1.append(words[i:i+150])
a=[0]*200

##url2 ALice's Adeventures in wonderland
url1 = "http://www.gutenberg.org/cache/epub/345/pg345.txt"
response = request.urlopen(url1)
law = response.read().decode('utf8')
#performing stripping of the text
law1=law.rstrip()
#converting the text into lowercase
law1=law1.lower()
#code for stopwords
stop_words1 = set(stopwords.words('english')) 
#performing Tokenisation of the code
word_tokens1 = word_tokenize(law1) 
filtered_sentence1 = [w for w in word_tokens1 if not w in stop_words1] 
filtered_sentence1 = [] 
for w in word_tokens1: 
    if w not in stop_words1: 
        filtered_sentence1.append(w)
wordsAlice = [word for word in filtered_sentence1 if word.isalpha()]
detokenizer1 = Detok()
wordsAlice1=detokenizer1.detokenize(wordsAlice)
x2=[]

#For Loop for the segmentation into 200 rows and 150 columns
for i in range (0,200*150,150):
     x2.append(words[i:i+150])
b=[1]*200


#url3 Democracy and Education: An Introduction to the Philosophy of Education by Dewey
url2="http://www.gutenberg.org/cache/epub/26361/pg26361.txt"
response = request.urlopen(url2)
saw = response.read().decode('utf8')
#stripping the text
saw1=saw.rstrip()
#converting the text to lower case
saw1=saw1.lower()
#removing all the stopwords
stop_words2 = set(stopwords.words('english')) 
#tokenize the text
word_tokens2 = word_tokenize(saw1)
 
filtered_sentence2 = [w for w in word_tokens2 if not w in stop_words2] 
filtered_sentence2 = [] 
for w in word_tokens2: 
    if w not in stop_words2: 
        filtered_sentence2.append(w) 
wordsDemo = [word for word in filtered_sentence2 if word.isalpha()]
detokenizer2 = Detok()
wordsDemo1=detokenizer2.detokenize(wordsDemo)
x3=[]

#For Loop for the segmentation into 200 rows and 150 columns
for i in range (0,200*150,150):
     x3.append(words[i:i+150])
c=[2]*200

#Merging Books and Labels
Merge=x1+x2+x3


import pandas as pd

#creating the numpy as array of the Merged list
Documents=np.asarray(Merge)
#Merging all the lables first 200 for 0, again 200 for 1, again 200 for 2
Label=a+b+c
#creating a numpy as array for the lables
Label=np.asarray(Label)
#appending the lables as well 150 segments of the text together
#DM1=np.append(X1,a[:,None],axis-=1)
#DM=np.append(Documents,Label[:,None],axis=1)
#DocMerge=np.append(Documents,Label[:,None],axis=1)
doc=np.c_[Documents,Label]
#print(doc)
#DocMerge1=DocMerge
#DM=DM1+DM2+DM3


#importing the Random Package
import random
#shuffling the merged document
np.random.shuffle(doc)


AfterLabel=doc[:,-1:]
AfterDoc=doc[:,:150]

##converting to dataframe for segments
df=pd.DataFrame(AfterDoc)
#converting to dataframe for labels
y=pd.DataFrame(AfterLabel)


df=[" ".join(c) for c in df.values]
df=pd.DataFrame(df)

#importing various modules of scikit-learn's
from sklearn.model_selection import train_test_split

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from  sklearn.metrics  import accuracy_score

count_vect = CountVectorizer()
X_train_counts = (count_vect.fit_transform(df[0]))
tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
X_train, X_test, y_train, y_test = train_test_split(X_train_counts,y,test_size=.30 )
clf = MultinomialNB().fit(X_train_tfidf, y)
predicted=clf.predict(X_test)
print(predicted)
print(accuracy_score(y_test,predicted))

#Nikhil
##Model Building and Evaluation (TF-IDF)
#from sklearn.naive_bayes import MultinomialNB
#from sklearn import metrics
#
#
## Model Generation Using Multinomial Naive Bayes
#clf = MultinomialNB().fit(X_train, y_train.values.ravel())
#predicted= clf.predict(X_test)
#print("MultinomialNB Accuracy:",metrics.accuracy_score(y_test, predicted))



from sklearn.svm import LinearSVC
model = LinearSVC()

model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print(accuracy_score(y_test,y_pred))


from sklearn.tree import DecisionTreeClassifier  
classifier = DecisionTreeClassifier()  
classifier.fit(X_train, y_train) 
y_pred1 = classifier.predict(X_test) 
print(accuracy_score(X_train,y))


#Bagofwords
#test_bag=['Benjamin',
#          'Dracula']
#np_bag=np.array(test_bag)
##
##np_bag1=pd.DataFrame(np_bag)
#count_vect = CountVectorizer()
#X_train_counts = (count_vect.fit_transform(df[0]))
##bagtest=(count_vect.transform(np_bag1[0]))
###print(bagtest)
#X_train, X_test, y_train, y_test = train_test_split(X_train_counts,y,test_size=.3 )
##clf = MultinomialNB().fit(X_train, y_train) 
##predicted=clf.predict(X_test)
##print(predicted)
##print(accuracy_score(y_test,predicted))
#
##k neighbour BOW
#from sklearn.neighbors import KNeighborsClassifier  
#classifier = KNeighborsClassifier(n_neighbors=5)  
#cl=classifier.fit(X_train, y_train)
#predict=cl.predict(X_test)
#print(predict)
#print(accuracy_score(y_test,predict))
#TFIDF

#score = metrics.accuracy_score(y_test, pred)
##print(y_test)
#from sklearn.metrics import classification_report
#print(classification_report(y_test, predicted))


#print(predicted)


##TFIDF neighbour 
#from sklearn.neighbors import KNeighborsClassifier  
#classifier = KNeighborsClassifier(n_neighbors=5)  
#c1=classifier.fit(X_train, y_train)
#predic=c1.predict(X_test)
#print(accuracy_score(y_test,predic))
